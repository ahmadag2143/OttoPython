# Otto ETL Pipeline

A production-ready ETL (Extract, Transform, Load) pipeline for processing product sales data and generating revenue analytics for business intelligence reporting.

## ğŸ¯ Overview

The Otto ETL Pipeline transforms raw product and sales data into a comprehensive revenue dataset suitable for PowerBI reports and business analytics. It processes daily sales data across all products, filling gaps for products with no sales to provide complete business visibility.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Product   â”‚    â”‚    Sales    â”‚    â”‚  Calendar   â”‚
â”‚   Database  â”‚    â”‚  Database   â”‚    â”‚ (Generated) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                  â”‚                  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                    â”‚    ETL    â”‚
                    â”‚ Transform â”‚
                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                          â”‚
                    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                    â”‚  Revenue  â”‚
                    â”‚   Table   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

- **Data Extraction**: Reads from product, sales, and calendar tables
- **Data Validation**: Dual-layer validation with Pandera (schema) and Pydantic (business rules)
- **Data Transformation**: Creates complete product-date grid with revenue calculations
- **Configuration Management**: Environment-based configuration for multi-stage deployments
- **Centralized Logging**: Comprehensive observability throughout the pipeline

## ğŸ“Š Data Model

### Input Tables
- **product**: Product catalog with SKU, description, and pricing
- **sales**: Transaction records with order details and quantities
- **calendar**: Date dimension (auto-generated if empty)

### Output Table
```sql
CREATE TABLE revenue (
    sku_id INTEGER,     -- Product identifier
    date_id DATE,       -- Transaction date
    price REAL,         -- Product price
    sales INTEGER,      -- Quantity sold (0 if no sales)
    revenue REAL        -- Calculated revenue (price Ã— sales)
);
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.9+
- SQLite database with product and sales tables

### Installation

1. **Clone and setup**:
```bash
git clone <repository-url>
cd otto
```

2. **Install dependencies**:
```bash
pip install -e .
```

3. **Configure environment** (optional):
```bash
cp .env.example .env
# Edit .env with your settings
```

4. **Run the pipeline**:
```bash
python main.py
```

## âš™ï¸ Configuration

The pipeline uses environment-based configuration for flexible deployment across different environments.

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `DATABASE_URL` | `product_sales.db` | Database connection string |
| `START_DATE` | `2025-01-01` | Start date for data processing |
| `END_DATE` | `2025-01-31` | End date for data processing |
| `LOG_LEVEL` | `INFO` | Logging level (DEBUG, INFO, WARNING, ERROR) |
| `BATCH_SIZE` | `10000` | Processing batch size |
| `ENABLE_PYDANTIC_VALIDATION` | `true` | Enable row-level validation |
| `ENABLE_PANDERA_VALIDATION` | `true` | Enable schema validation |
| `ENVIRONMENT` | `development` | Deployment environment |

### Configuration Examples

**Development**:
```bash
ENVIRONMENT=development
LOG_LEVEL=DEBUG
DEBUG=true
```

**Production**:
```bash
ENVIRONMENT=production
LOG_LEVEL=WARNING
DATABASE_URL=postgresql://user:pass@host:port/db
MAX_RETRIES=5
```

## ğŸƒâ€â™‚ï¸ Usage

### Basic Usage
```python
from otto.main import main
main()
```

### Programmatic Usage
```python
from otto.config import config
from otto.etl import run_etl
from otto.db_utils import get_connection, read_table

# Configure for your environment
config.start_date = "2024-01-01"
config.end_date = "2024-01-31"

# Run ETL
with get_connection(config.database_url) as conn:
    products_df = read_table(conn, "product")
    sales_df = read_table(conn, "sales")

    result_df = run_etl(products_df, sales_df, calendar_df)
    print(f"Generated {len(result_df)} revenue records")
```

## ğŸ§ª Testing

### Run All Tests
```bash
pytest tests/ -v
```

### Run Specific Test Categories
```bash
# Test data models
pytest tests/test_models.py -v

# Test ETL logic
pytest tests/test_etl.py -v

# Test configuration
pytest tests/test_config.py -v
```

### Test Coverage
```bash
pytest --cov=otto tests/
```

## ğŸ“ Project Structure

```
otto/
â”œâ”€â”€ src/otto/                   # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py               # Configuration management
â”‚   â”œâ”€â”€ db_utils.py             # Database utilities
â”‚   â”œâ”€â”€ etl.py                  # ETL transformation logic
â”‚   â”œâ”€â”€ logging_config.py       # Centralized logging
â”‚   â”œâ”€â”€ main.py                 # Application entry point
â”‚   â”œâ”€â”€ models.py               # Pydantic data models
â”‚   â”œâ”€â”€ schemas.py              # Pandera validation schemas
â”‚   â””â”€â”€ utils.py                # Utility functions
â”œâ”€â”€ tests/                      # Test suite
â”‚   â”œâ”€â”€ test_config.py
â”‚   â”œâ”€â”€ test_etl.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â”œâ”€â”€ test_schemas.py
â”‚   â””â”€â”€ test_utils.py
â”œâ”€â”€ docs/                       # Documentation
â”‚   â””â”€â”€ CONFIGURATION.md
â”œâ”€â”€ .env                        # Environment configuration
â”œâ”€â”€ .gitignore                  # Git ignore rules
â”œâ”€â”€ .flake8                     # Code style configuration
â”œâ”€â”€ main.py                     # CLI entry point
â”œâ”€â”€ pyproject.toml              # Project configuration
â”œâ”€â”€ requirements.txt            # Dependencies
â””â”€â”€ README.md                   # This file
```

## ğŸ” Data Quality & Validation

The pipeline implements multiple validation layers:

### Schema Validation (Pandera)
- **DataFrame-level validation**: Ensures data types and constraints
- **Column validation**: Checks for required fields and data integrity
- **Business rule validation**: Enforces domain-specific constraints

### Business Rule Validation (Pydantic)
- **Row-level validation**: Validates individual records
- **Type safety**: Ensures data type consistency
- **Custom validators**: Business logic validation

### Example Validations
```python
# Price must be positive
price: float = Field(..., gt=0)

# SKU ID must be positive integer
sku_id: int = Field(..., gt=0)

# Sales quantity cannot be negative
sales: int = Field(..., ge=0)
```

## ğŸ“ˆ Performance Features

- **Efficient data processing**: Vectorized operations with pandas
- **Memory optimization**: Configurable batch processing
- **Error handling**: Comprehensive error handling with retries
- **Scalable architecture**: Designed for large datasets

### Performance Metrics
- **Data throughput**: ~90K+ sales records processed in seconds
- **Memory efficiency**: Configurable batch sizes for large datasets
- **Fault tolerance**: Automatic retry mechanisms with exponential backoff

## ğŸ”§ Development

### Setup Development Environment
```bash
# Install in development mode
pip install -e ".[dev]"

# Install pre-commit hooks
pre-commit install

# Run code quality checks
flake8 src/ tests/
```

### Code Style
- **PEP 8 compliance** with 150-character line limit
- **Type hints** throughout codebase
- **Comprehensive docstrings** for all functions
- **Import organization** following isort standards

### Adding New Features
1. Create feature branch
2. Add comprehensive tests
3. Update documentation
4. Ensure all quality checks pass
5. Submit pull request

## ğŸš¢ Deployment

### Docker Deployment
```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY . .
RUN pip install -e .

ENV ENVIRONMENT=production
CMD ["python", "main.py"]
```

### Kubernetes Deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otto-etl
spec:
  template:
    spec:
      containers:
      - name: otto-etl
        image: otto-etl:latest
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: otto-secrets
              key: database-url
```

### CI/CD Pipeline
The project supports automated CI/CD with:
- **Automated testing** on multiple Python versions
- **Code quality checks** with flake8 and pytest
- **Security scanning** with safety and bandit
- **Automated deployment** to staging and production

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes with tests
4. Ensure all tests pass (`pytest tests/ -v`)
5. Commit your changes (`git commit -m 'Add amazing feature'`)
6. Push to the branch (`git push origin feature/amazing-feature`)
7. Open a Pull Request

### Contribution Guidelines
- **Test coverage**: Maintain >90% test coverage
- **Documentation**: Update documentation for new features
- **Code style**: Follow existing code style and conventions
- **Type hints**: Add type hints for all new functions

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

### Common Issues

**Import Errors**:
```bash
# Reinstall in development mode
pip install -e .
```

**Database Connection Issues**:
```bash
# Check database path in configuration
python -c "from otto.config import config; print(config.database_url)"
```

**Test Failures**:
```bash
# Run with verbose output
pytest tests/ -v -s
```

### Getting Help
- ğŸ“š Check the [documentation](docs/)
- ğŸ› [Report bugs](issues/new?template=bug_report.md)
- ğŸ’¡ [Request features](issues/new?template=feature_request.md)
- ğŸ’¬ [Join discussions](discussions/)

## ğŸ“Š Monitoring & Observability

The pipeline provides comprehensive logging and monitoring capabilities:

### Logging Features
- **Structured logging** with configurable levels
- **Performance metrics** for each processing step
- **Error tracking** with full stack traces
- **Business metrics** (records processed, validation results)

### Example Log Output
```
2025-08-08 17:53:06,132 INFO otto Starting ETL transformation
2025-08-08 17:53:06,180 INFO otto Preprocessing sales data: adding date_id and aggregating sales
2025-08-08 17:53:06,294 INFO otto ETL transformation complete. Output rows: 31000
2025-08-08 17:53:06,372 INFO otto Pipeline completed. Output written to 'revenue' table.
```

---

**Built with â¤ï¸ for scalable data processing at OTTO**
